\documentclass[chi_draft]{sigchi}

% Use this section to set the ACM copyright statement (e.g. for
% preprints).  Consult the conference website for the camera-ready
% copyright statement.

% Copyright
%\CopyrightYear{2016}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}
% DOI
%\doi{http://dx.doi.org/10.475/123_4}
% ISBN
%\isbn{123-4567-24-567/08/06}
%Conference
%\conferenceinfo{CHI'16,}{May 07--12, 2016, San Jose, CA, USA}
%Price
%\acmPrice{\$15.00}

% Use this command to override the default ACM copyright statement
% (e.g. for preprints).  Consult the conference website for the
% camera-ready copyright statement.

%% HOW TO OVERRIDE THE DEFAULT COPYRIGHT STRIP --
%% Please note you need to make sure the copy for your specific
%% license is used here!
\toappear{
% Permission to make digital or hard copies of all or part of this work
% for personal or classroom use is granted without fee provided that
% copies are not made or distributed for profit or commercial advantage
% and that copies bear this notice and the full citation on the first
% page. Copyrights for components of this work owned by others than ACM
% must be honored. Abstracting with credit is permitted. To copy
% otherwise, or republish, to post on servers or to redistribute to
% lists, requires prior specific permission and/or a fee. Request
% permissions from \href{mailto:Permissions@acm.org}{Permissions@acm.org}. \\
% \emph{CHI '16},  May 07--12, 2016, San Jose, CA, USA \\
% ACM xxx-x-xxxx-xxxx-x/xx/xx\ldots \$15.00 \\
% DOI: \url{http://dx.doi.org/xx.xxxx/xxxxxxx.xxxxxxx}
}

% Arabic page numbers for submission.  Remove this line to eliminate
% page numbers for the camera ready copy
% \pagenumbering{arabic}

% Load basic packages
\usepackage{balance}       % to better equalize the last page
\usepackage{graphics}      % for EPS, load graphicx instead 
\usepackage[T1]{fontenc}   % for umlauts and other diaeresis
\usepackage{txfonts}
\usepackage{mathptmx}
\usepackage[pdflang={en-US},pdftex]{hyperref}
\usepackage{color}
\usepackage{booktabs}
\usepackage{textcomp}

% Some optional stuff you might like/need.
\usepackage{microtype}        % Improved Tracking and Kerning
% \usepackage[all]{hypcap}    % Fixes bug in hyperref caption linking
\usepackage{ccicons}          % Cite your images correctly!
% \usepackage[utf8]{inputenc} % for a UTF8 editor only

% If you want to use todo notes, marginpars etc. during creation of
% your draft document, you have to enable the "chi_draft" option for
% the document class. To do this, change the very first line to:
% "\documentclass[chi_draft]{sigchi}". You can then place todo notes
% by using the "\todo{...}"  command. Make sure to disable the draft
% option again before submitting your final document.
\usepackage{todonotes}
\usepackage{paralist}
\usepackage{lscape}

% Paper metadata (use plain text, for PDF inclusion and later
% re-using, if desired).  Use \emtpyauthor when submitting for review
% so you remain anonymous.
\def\plaintitle{Clash of Crowds: Understanding Human \\ Text Classification though Crowdsourcing Games}
\def\plainauthor{First Author, Second Author, Third Author,
  Fourth Author, Fifth Author, Sixth Author}
\def\emptyauthor{}
\def\plainkeywords{Authors' choice; of terms; separated; by
  semicolons; include commas, within terms only; required.}
\def\plaingeneralterms{Documentation, Standardization}

% llt: Define a global style for URLs, rather that the default one
\makeatletter
\def\url@leostyle{%
  \@ifundefined{selectfont}{
    \def\UrlFont{\sf}
  }{
    \def\UrlFont{\small\bf\ttfamily}
  }}
\makeatother
\urlstyle{leo}

% To make various LaTeX processors do the right thing with page size.
\def\pprw{8.5in}
\def\pprh{11in}
\special{papersize=\pprw,\pprh}
\setlength{\paperwidth}{\pprw}
\setlength{\paperheight}{\pprh}
\setlength{\pdfpagewidth}{\pprw}
\setlength{\pdfpageheight}{\pprh}

\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\usepackage{paralist}
\newcommand{\pro}[1]{\text{#1}}
\newcommand{\con}[1]{\text{#1}}

% Make sure hyperref comes last of your loaded packages, to give it a
% fighting chance of not being over-written, since its job is to
% redefine many LaTeX commands.
\definecolor{linkColor}{RGB}{6,125,233}
\hypersetup{%
  pdftitle={\plaintitle},
% Use \plainauthor for final version.
%  pdfauthor={\plainauthor},
  pdfauthor={\emptyauthor},
  pdfkeywords={\plainkeywords},
  pdfdisplaydoctitle=true, % For Accessibility
  bookmarksnumbered,
  pdfstartview={FitH},
  colorlinks,
  citecolor=black,
  filecolor=black,
  linkcolor=black,
  urlcolor=linkColor,
  breaklinks=true,
  hypertexnames=false
}

% create a shortcut to typeset table headings
% \newcommand\tabhead[1]{\small\textbf{#1}}

% End of preamble. Here it comes the document.
\begin{document}

\title{\plaintitle}

\numberofauthors{1}
\author{%
  \alignauthor{Tongshuang Wu \quad Jim Chen \quad Chenglong Wang\\
    \affaddr{University of Washington, USA}\\
    \email{\textsf{\{wtshuang, cqz, clwang\}@cs.washington.edu}}
    }\\
}

\maketitle

\begin{abstract}
While feature extractions and selections have become popular in the field of interactive Machine Learning, 
little has been known how effective such human intersections are. 
In this work, we indirectly learn and verify latent features humans actually use by gamifying the simultaneous labeling and feature selection procefure.
By comparing four crowdsourcing games with a benchmark system and between each other, we hope to explore if and how human judgments could effectively inform machine learning models.
\end{abstract}

%\category{H.5.m.}{Information Interfaces and Presentation
%  (e.g. HCI)}{Miscellaneous} \category{See
%  \url{http://acm.org/about/class/1998/} for the full list of ACM
%  classifiers. This section is required.}{}{}

%\keywords{\plainkeywords}

\section{Introduction}

%Identifying predictive features is key to creating effective machine learning classifiers. 
Interactive feature selection has become one of the mainstreams in interactive Machine Learning (iML).
Current classification systems have been taking advantage of human judgments to select predictive features to build effective machine learning models~\cite{brooks2015featureinsight, cheng2015flock}. 
\todo{For instance??}
While researchers have argued that human could contribute their domain knowledge to the models~\cite{stumpf2009interacting}, studies have frequently proved that human intersections are not as effective as we have hoped. 
In fact, such intersections have caused the performances of multiple models to decrease~\cite{fiebrink2011human,stumpf2009interacting}.
One of the potential barriers here is that we have little have been known how effective are the human inputs.
This is because of two reasons: 

First, current feature selection procedures may not reflect the real reasoning procedures of feature engineers.
Prior work (e.g.,~\cite{trivedi2015interactive}) seeks to understand features that human use by mapping users' mental model into concrete words in the document. The approach used to collected such feature words (i.e., ``human features''), take sentiment analysis for example, is to directly ask users to (1) label a document and (2) then select words associated with this decision it. 
While such techniques are straightforward and easy to collect, it is unclear whether features collected in this way suffer from the ``hindsight bias'' effect of human labelers. 
 ~\cite{gazzaniga2013integrated} has proved that people are generally poor at introspecting on the signals they use to make decisions.
In other words, post-hoc word selection could lead users to ``make up'' reasons (i.e., words) that they believe best explain their completed labeling decision, rather than the ones that reflect their instincts. If such effect exists, collected labels and features are not guaranteed to be naturally paired.

Second, we are yet to understand if computers and humans proceed a classification task in the same way. 
Most of the state-of-art iML systems use linear models like Naive Bayes~\cite{kulesza2015principles, kulesza2011oriented, amershi2012regroup} as their underlying algorithm because of its interpretability.
Such algorithms are well known for using the occurrence frequencies of n-grams. 
However, we are yet to understand how humans make their judgments, and if inputing their decision making procedure into the originally fully automated training process is indeed beneficial.
In fact, humans could determine the effectiveness of certain features based on evidences other than occurrence frequencies
This means the human-chosen features could be dramatically different from, and potentially not understandable for machine learning models.

Understanding the human procedure is crucial for building effective iML systems.
By comparing and articulating the differences between the classification process of humans and computers, we could fulfill the mysterious gap between ``what humans could provide'' and ``what computers could intake''.
Instead of merely providing features that us humans identify as ``suppose to work'' and iterate through the tedious ``trial-and-error'' procedure, such understanding could guide humans to pinpointedly contribute their domain knowledge in a way that the machines could most efficiently acquire. 
Our project, therefore, seeks to understand the human feature engineering procedure.
We choose to work in the domain of sentiment analysis, as it is one of the most important and fundamental classification task, and that it is intuitive enough so that human understandings will not obstruct our learning.
Specifically, we aim to explore two things: (1) how do human understandings for document sentiment map onto single words, and (2) if we could effective extract real ``human features'', or the understanding-word mappings mentioned, through certain workflows.

To achieve this, we transfer the feature extraction workflow into four different games, namely the collaborative games \emph{Bingo} and \emph{Hangman-add}, and the competitive ones \emph{Hangman-subtract} and \emph{Censor}.
All the games support simultaneous collection of document labels and features to ensure feature-label mapping.
By manipulating the users' possible inputs and their access to information in each game, we try to indirectly learn and verify latent features humans actually use. 
Deploying the games to Amazon Mechanical Turk, we prove that our games could successfully help collect various sets of human features (e.g., the smallest set that could facilitate the decision making versus the most inclusive set that makes an instance classifiable for humans), and evaluate the how different incentives affect participants' feature extraction. 

We conclude through the analysis that \todo{Summarize the analysis result.}


Our contributions are as follows:
\begin{compactitem}
	\item \todo{Statement of Contributions}
\end{compactitem}


%In order to design interactive machine learning systems that better coordinate human and computer, system designers need to have a good understanding on how human and computer make their decisions \todo{add some citation to justify this}. For example, in order to design a interactive machine learning system for text sentiment classification, one needs to know which words in the text are features used by humans and computers to make their decisions (whether a document is positive or negative). While features used by machine learning algorithms, e.g., n-gram model, are well understood, little is know about what are the text features used by human in their decision making process. Our project seeks to understand the human labeling process (the connection between labels created by human and feature words influence them most in the labeling process) in the context of \emph{text sentiment labeling tasks}. 


%In order to collect more meaningful features and provides better explanation of the these features, our key insight is to collect features along with labels during the labeling procedure though interactive games. Concretely, we designed a set of 5 crowd sourcing games: \emph{DirectAnnotation}, \emph{Bingo}, \emph{HangmanAdd}, \emph{HangmanSub} and \emph{Censor}. While these games share the same goal, players in different games have different ways to access information in target documents. These controlled variance in the game lets us both collecting and verifying features and labels for every target document, as we will present later in Section~\ref{}. Though our implementation and deployment of the game, we observed the following set of facts that can potentially be used to guide interactive machine learning system design. \todo{add a few descriptions}


%The rest of the report structured as follows: (1) the text classification task we aim to solve (Section~\ref{sec:task}), (2) our game designs, (3) our deployment of the games and evaluation.


%!TEX root=./proposal.tex

\section{Related Work}
\label{sec:relatedWork}
%What existing understanding of the problem has been developed?
%For a research proposal, this will briefly cover the most important related work in the space you are exploring.
%For a design proposal, this will introduce existing solutions, why they fall short, and the potential opportunity.

\subsection{Feature Engineering in iML}
Feature engineering is one of the two main approaches to manipulate and adjust machine learning models.
Feature engineering systems would ask users to specify the included features or the feature weights based on their domain knowledge: INFUSE~\cite{krause2014infuse} visualize results of multiple feature selection algorithms to help users comprehensively understand select the most effective features, FeatureInsight~\cite{brooks2015featureinsight} supports building new dictionary features (semantically related groups of words) for text classification problems, and EMR VisWeb~\cite{trivedi2015interactive} enables clinical researchers to review and annotate clinical text.
Besides such end-user centered systems, some studies also try to enhance Machine Learning practitioners' experience browsing the data. For instance, Hoffmann et al. designed multiple strategies to supports experts to rapidly brainstorm rules for information extraction~\cite{hoffmann2015extreme}.

While such work makes users engineer features more efficiently, the effectiveness is yet to be improved.
Some work~\cite{fiebrink2011human,stumpf2009interacting, patel2008investigating} has proved that human intersections in general, including feature selection, sometimes tend to hurt the performance of a classifier. 
The proposed problems is that users' manipulation could either over- or under- constraint the feature space.
The more essential problem here is the mismatch between the features provided by humans and the ones machines need to learn about. 
As the features needed by the machines have been well studied with automated feature selection algorithms~\cite{empirical}, we focus on understanding human feature selection procedure here to fill the gap. 

The most related work to ours is Flock~\cite{cheng2015flock}, which presents a technique to crowdsource features for machine-learning classifiers.
In their paper, Cheng et al. made a similar argument to ours that people struggle to articulate what led them to make their decision, and therefore proposed to collect crowd workers' reasoning with free-text.
While they provided a novel approach to generate features, the open-ended summary allows too much freedom for building a real model, resulting in subjective and non-machine-extractable features.
We instead gamify the labeling and feature selection procedure, so to collect ``human features'' in a constrained form (in this case, individual words) that can be directly incorporated into machines.


\subsection{Human Computation Games}

Human computation games are designed to solve problems that are hard for computers, where a problem is encoded into a game and the solution is drawn from players movements in the game. Prior computation games are designed for labeling multimedia documents (e.g., image, video, music)~\cite{vonAhn:2004:LIC:985692.985733,ho2009kisskissban,vonAhn:2006:PGL:1124772.1124782,Seneviratne:2010:IFI:1743384.1743473,law2007tagatune}, collection recommendation~\cite{walsh2010curator}, collecting common sense~\cite{von2006verbosity} and more. 

In order to obtain better solutions from the user, such games are designed with both collaborative and competitive elements. Collaborative elements are designed to cross validate solutions from different users, where multiple plays must make an agreement on the game solution to achieve the goal~\cite{vonAhn:2006:PGL:1124772.1124782}, and competitive elements~\cite{ho2009kisskissban} are demonstrated to be effective against cheats and vagueness in collaborations, where different groups have opposite goals and they are supposed to optimize their solution while fighting against others.

Our proposed project divides players into two competitive groups and support in-group collaboration and between-group competition, which resembles the design of KissKissBan~\cite{ho2009kisskissban} (KKB). In contrast to KKB, we aim to engage large dynamic groups and have dynamic working sets which creates unique challenges in game design. Furthermore, our goal involves both labeling and feature selection instead of brainstorming (annotation). This implies finding minimal constraints as opposed to optimizing diversity. Selected features can be viewed as an explanation of the labeling result, which allows a built-in evaluation mechanism for both the challenge (sample) and the workers. 




\section{Task Selection}
\label{sec:task}

The text classification task used in our paper is sentiment labeling for IMDB movie reviews. Labeling sentiment of movie reviews is a challenge task since (1) many reviews contain humorous or ironic elements that makes the movie sentiment orientation tricky to identify, and (2) movie reviews are self labeled: sentiment labeling of them are directly posted along with the review, which makes the training data and ground truth easy to obtain.

\todo{Add some more description of the review?}

\section{Game Design} 

We have designed the following fives games to collect text sentiment labels and features from players; their interfaces are shown in Figure~\ref{fig:game-interface}.

\begin{figure*}[t]
\includegraphics[width=\linewidth]{gamepreview.pdf}
\caption{Interface of DirectAnnotate, Bingo, HangmanAdd and HangmanSubtract}
\label{fig:game-interface}
\end{figure*}

\begin{itemize}
\item \emph{DirectAnnotate}. This is the base game where the player directly labels document sentiment and selects words that influence decision making (informative words).
\item \emph{Bingo}. This is the game where two players coop on labeling document sentiment and extract informative words. The game requires labels collected from the team are the same, and the game is finished when two players reach an agreement on four informative words. 
\item \emph{HangmanAdd}. Players in this game are assigned as annotator and guesser. The annotator label the document and provides a list of ranked informative words to assist the guesser to guess the document sentiment. Game result counts on how many words are used and whether the guesser result agree with the annotator's label.
\item \emph{HangmanSubtract}. This is a competitive game where the two players are assigned as roles masker and guesser. The masker is asked to label the document and mask informative words that are important to his decision. The masker's goal is to hinder the guess to figure out the correct the document sentiment and the guesser is asked to guess document based on the masked document. (The guesser can unmask words with the reduction on the payment bonus and the masker earn more if labeled less but more effective.) 
\item \emph{Censor}. \todo{add its content}
\end{itemize}

Our games are designed based the three criteria below, and our game design details are presented in Table~\ref{table:game}.

\begin{itemize}\itemsep-1pt
  \item \emph{Data quality}: Players of the game should be table to collect less noisy or biased labels and features through the game, and preferably, collected features can be verified.
  \item \emph{Difficulty}: The game should be fairly intuitive to play and should not overwhelm the players. 
  \item \emph{Compatible incentive}: The incentives for different players, either in competitive or collaborative games, should be compatible such that neither sides have shortcuts and could play in a fair setting.
\end{itemize}

\begin{table*}
\footnotesize
\centering
\begin{tabular}{|p{0.08\paperwidth} |p{0.07\paperwidth} |p{0.13\paperwidth} |p{0.3\paperwidth} |p{0.18\paperwidth}|}
\hline
\textbf{Game\/System} & \textbf{Player} & \textbf{Mechanism} & \textbf{Pros\&Cons} & \textbf{Compatible Incentive} 
\\  \hline 
%%
\tabincell{l}{Direct\\Annotation\\(Baseline)}
& 1P & Directly ask users to choose features they used. 
& \begin{compactitem}
 \item[$+$] \pro{Access to the full doc}
 \item[$-$] \con{* Hindsight bias}
 \item[$-$] \con{* No checks}
 \end{compactitem}
& N/A (not a game) 
\\  \hline 
%%
Bingo 
& \tabincell{l}{2P,\\Collab} 
& Two users annotate one document. Score when they label the same feature. 
& \begin{compactitem}
 \item[$+$] \pro{* Less noisy}
 \item[$+$] \pro{Access to the full doc} 
 \item[$-$] \con{* Hindsight bias}
 \item[$-$] \con{* Low representation of individual users}
 \end{compactitem}
& Reward Mutual agreement
\\ \hline 
%% 
\tabincell{l}{Hangman,\\\emph{Additive}}
& \tabincell{l}{2P,\\Collab} 
& \textbf{Guesser} asks for a word from annotator. \textbf{Annotator} returns a word to the guesser.
& \begin{compactitem}
 \item[$+$] \pro{* Mediate hindsight bias}
 \item[$+$] \pro{Engaging} 
 \item[$-$] \con{Takes more time}
 \item[$-$] \con{Guesser cannot access document structure}
 \end{compactitem}
& \begin{compactitem}[-]
 \item Reward agreement
 \item Penalize queries used
 \end{compactitem}
\\ \hline
%%
Censor
& \tabincell{l}{2P,\\Compete} 
& \textbf{Censors} mask words in a document.
\textbf{Identifiers} label the censored document.
& \begin{compactitem}
 \item[$+$] \pro{* Keep doc structure}
 \item[$+$] \pro{* Mediate hindsight bias} 
 \item[$+$] \pro{Engaging and easy}
 \item[$-$] \con{Can't reuse identifiers}
 \item[$-$] \con{Need to retain censors}
 \end{compactitem}
& \tabincell{l}{\textbf{Censors}: \\Reward fooling users\\\textbf{Identifiers}: \\Reward for being correct}
\\ \hline
%%
\tabincell{l}{Hangman,\\\emph{Subtractive}}
& \tabincell{l}{2P,\\Compete} 
& \textbf{Censors} remove a set of words from a document.
\textbf{Guesser} queries one word at a time (can choose position).
& \begin{compactitem}
 \item[$+$] \pro{* Keep doc structure}
 \item[$+$] \pro{* Mediate hindsight bias} 
 \item[$+$] \pro{Engaging and easy}
 \item[$-$] \con{Can't reuse guessers}
 \item[$-$] \con{More mental load balancing prices}
 \end{compactitem}
& \tabincell{l}{\textbf{Censors}: \\Reward fooling users\\\textbf{Identifiers}: \\Reward for being correct}
\\ \hline
\end{tabular}
\caption{Designed games. Pros and Cons with ``*'' are the attributes affecting data quality. In the table, \emph{Game Mode} indicates the number of the players in the game and whether it is collaborative or competitive, \emph{Mechanism} presents how labels and features are obtained from the player through the game, \emph{Pros\&Cons} shows what is the game designs strength and weakness and \emph{Incentive Mechanism} indicates what makes the game incentive to the players.}
\label{table:game}
\end{table*}

\section{Implementation \& Deployment} Our games are deployed through....\todo{add details}

\section{Data Analysis} 

\subsection{Collected Features} 
The set of features that can be collected from different games are presented below. 

\begin{center}
\begin{tabular}{|c|cccc|}
\hline
Data  & Ann. & Bin. &  H.Add  &   H.Sub   \\
 \hline
Labels-full & \textbullet & \textbullet & \textbullet & \textbullet  \\
Labels-verify &  & $\circ$ & \textbullet & \textbullet  \\
Features-init & \textbullet & \textbullet & \textbullet & \textbullet  \\
Features-rank &  & \textbullet & \textbullet & \textbullet  \\
Features-refine &  &  &  & \textbullet  \\
Features-verify &  & \textbullet & \textbullet & \textbullet  \\\hline
\end{tabular}
\end{center}

{\emph{Labels-full} refers to whether we can extract sentiment label from the game, \emph{labels-verify} refers to whether the game supports different players to verify their label result, \emph{feature-init} refers to whether the game obtains features provided by the gamer, \emph{feature-rank} refers to whether obtained features are ranked, \emph{feature-refine} refers to whether the player can refine initially collected feature, and \emph{feature-verify} refers to whether features are verified though multiple players.}

\section{Conclusion}

\section{Future Work}


% Balancing columns in a ref list is a bit of a pain because you
% either use a hack like flushend or balance, or manually insert
% a column break.  http://www.tex.ac.uk/cgi-bin/texfaq2html?label=balance
% multicols doesn't work because we're already in two-column mode,
% and flushend isn't awesome, so I choose balance.  See this
% for more info: http://cs.brown.edu/system/software/latex/doc/balance.pdf
%
% Note that in a perfect world balance wants to be in the first
% column of the last page.
%
% If balance doesn't work for you, you can remove that and
% hard-code a column break into the bbl file right before you
% submit:
%
% http://stackoverflow.com/questions/2149854/how-to-manually-equalize-columns-
% in-an-ieee-paper-if-using-bibtex
%
% Or, just remove \balance and give up on balancing the last page.
%
%\balance{}

% REFERENCES FORMAT
% References must be the same font size as other body text.
\bibliographystyle{SIGCHI-Reference-Format}
\bibliography{sample}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
