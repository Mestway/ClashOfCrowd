\section{Proposed Approach}

One obvious way to collect people's opinions on certain uni-, bi-, or tri-gram features, is to provide sample sentences involving certain keywords to users, and directly collect their feedback on whether the keywords should be included in a classifier. However, users' behavior could vary greatly due differences in predicting corpus distribution from the examples or in understanding the behavior of a classifier. Another approach is to observe users' performance with a specific interactive machine learning system. The problem is then grounded in specific interactions, designs, etc. of that system and are likely not generalizable. We don't want prior knowledge of machines/algorithms to influence human decisions!

Therefore, in this project, we propose crowdsourcing human's feedback with an adversarial competitive game. In this game there will be two groups of users: 

\begin{compactenum}
\item \textbf{The Inspector Group}: This group tries to inspect examples and come up with labels.
\item \textbf{The Adversary Group}: This group tries to manipulate examples to confound the inspectors.
\end{compactenum}


In the context of sentiment analysis over tweets, adversaries can "censor" certain words from examples, while inspectors attempt to identify the sentiment. This is done without any pre-existing knowledge of the true labels. Essentially, when the Inspectors are reduced to picking by chance, we could claim that the adversaries have successfully collected important features.

Such a game has three benefits:
\begin{compactenum}
	\item \textbf{Elicit both the labels and the features} 
	\item \textbf{Collect information without bias due to user understanding of machines. }
	\item \textbf{Make the data collection process more engaging}
\end{compactenum}
 
With the collected data representing human's domain knowledge or context, we can then evaluate viability of applying interactive machine learning systems (and how/if human knowledge benefits such models).
